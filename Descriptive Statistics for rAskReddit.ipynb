{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an exploration of headlines from rAskReddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import sympy\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import random\n",
    "from os import listdir\n",
    "from typing import *\n",
    "\n",
    "class Headline:\n",
    "    touch_ups = dict( {\n",
    "        \"what?s\" : \"what's\",\n",
    "        \"who?s\" : \"who's\",\n",
    "        \"you?ve\" : \"you've\",\n",
    "        \"don?t\" : \"don't\",\n",
    "        \"redditor\" : \"reddit\",\n",
    "        \"reddit!\" : \"reddit\",\n",
    "        \"serie\" : \"series\"\n",
    "        })\n",
    "    \n",
    "    def __init__(self,raw_line,sanitize_fn: Callable[[str],str]) -> None:\n",
    "        self.words = [sanitize_fn(word) for word in raw_line.split(\" \")]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    def __str__(self):\n",
    "        return str(self.words)\n",
    "  \n",
    "    @staticmethod\n",
    "    def sanitize(s: str) -> str:\n",
    "        junk = \"!?:,\\\"\\'()\\n.0123456789*\"\n",
    "        s = s.lower().rstrip(junk).lstrip(junk)\n",
    "        #s = s.rstrip(junk)\n",
    "        ##s = s.lstrip(junk)\n",
    "    \n",
    "        \n",
    "        for k,v in Headline.touch_ups.items():\n",
    "            if s == k:\n",
    "                s = v\n",
    "                \n",
    "        return s\n",
    "    \n",
    "#helper functions\n",
    "def as_pct(data: float) -> str:\n",
    "    return f\"{data*100}%\"\n",
    "def printmd(string: str) -> str:\n",
    "    display(Markdown(string))\n",
    "    \n",
    "def ffile_type(path: str,ext:str ) -> str:\n",
    "    for entry in listdir(path):\n",
    "        if entry[-3:] == ext:\n",
    "            yield entry\n",
    "def read_chunk(path: str) -> List[str]:\n",
    "    for entry in ffile_type(path,\"csv\"):\n",
    "        with open(entry,'r') as file:\n",
    "            data = file.readlines()\n",
    "        yield data\n",
    "def sanitize(s: str) -> str:\n",
    "    junk = \"!?:,\\\"\\'()\\n.0123456789*\"\n",
    "    s = s.lower().rstrip(junk).lstrip(junk)\n",
    "    for k,v in Headline.touch_ups.items():\n",
    "        if s == k:\n",
    "            s = v\n",
    "                \n",
    "    return s\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 410813 headlines.\n",
      "410813 unique headlines.\n"
     ]
    }
   ],
   "source": [
    "headlines: List[str]\n",
    "with open(\"./all.csv\",'r') as file:\n",
    "    headlines = [Headline(line,sanitize) for line in file]\n",
    "\n",
    "print(f\"Read {len(headlines)} headlines.\")\n",
    "print(f\"{len(set(headlines))} unique headlines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_total = len(sentences)\n",
    "n_unique = len(unique_sentences)\n",
    "pct_unique = as_pct(n_unique / n_total)\n",
    "\n",
    "n_samples = len(listdir(\"./askreddit/\"))\n",
    "printmd(f\"**{n_samples}** samples\")\n",
    "printmd(f\"You have collected **{len(sentences)}** headlines, **{len(unique_sentences)}** (**{pct_unique}**) of which are unique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Relative Frequency of a Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [h_line.words for h_line in headlines]\n",
    "word = 'you'\n",
    "words = pd.Series(all_words)\n",
    "rel_freqs = words.value_counts(normalize=True)\n",
    "#rel_freqs[word] \n",
    "printmd(f\"The word **{word}** occurs about **{as_pct(rel_freqs[word])}** of the time.\")\n",
    "printmd(f\"The most common word in an r/AskReddit headline is **{rel_freqs.idxmax()}**, which occurs about **{as_pct(rel_freqs.max())}** of the time.\")\n",
    "printmd(f\"**Top ten most frequent words**\")\n",
    "print(f\"{rel_freqs[39:49]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Length of Headlines (Population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: FIX\n",
    "unique_sentence_lens = [len(s) for s in unique_sentences]\n",
    "mean = round(np.mean(unique_sentence_lens))\n",
    "median = round(np.median(unique_sentence_lens))\n",
    "printmd(f\"The average length of the headline on r/AskReddit is **{mean}** words\" )\n",
    "printmd(f\"The median length of the headline on r/AskReddit is **{median}** words\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "sentence_lens = sorted(sentence_lens)\n",
    "mu = np.mean(sentence_lens)\n",
    "std = np.std(sentence_lens)\n",
    "sentence_lens = [ (s - mu) / std for s in sentence_lens]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Length of Headlines (Population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sentence_lens = pd.Series(unique_sentence_lens)\n",
    "unique_sentence_lens.hist()\n",
    "plt.title(f\"Length of Headlines (words) on r/AskReddit (N={len(unique_sentences)})\")\n",
    "plt.show()\n",
    "printmd(f\"$\\sigma$ = {unique_sentence_lens.std()}\")\n",
    "printmd(f\"$\\mu$ = {unique_sentence_lens.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the Sample Distribution of the Sample Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_dict = [(item.words,len(item)) for item in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 30\n",
    "n_trials = int(1e3)\n",
    "sample_means = []\n",
    "over_time = []\n",
    "error = []\n",
    "\n",
    "\n",
    "\n",
    "sentence_dict = {}\n",
    "for i in range(0,n_trials):\n",
    "    sample_keys = random.sample(list(sentence_dict),sample_size)\n",
    "    sample = [sentence_dict[key] for key in sample_keys]\n",
    "    sample_sentence_lens = [len(s) for s in sample]\n",
    "    sample_sentence_len_mean = np.mean(sample_sentence_lens)\n",
    "    sample_means.append(sample_sentence_len_mean)\n",
    "    over_time.append(np.mean(sample_means))\n",
    "    error.append( abs(np.mean(sample_means) - unique_sentence_lens.mean()))\n",
    "plt.title(f\"Mean of Sampling Distribution of the Sample Mean vs. Number of Trials(Normalized; n={sample_size}; N={n_trials})\")\n",
    "sample_means = pd.Series(sample_means)\n",
    "plt.xlabel(\"Number of Trials\")\n",
    "plt.ylabel(\"Sample Mean (Cumulative)\")\n",
    "plt.plot(over_time)\n",
    "printmd(f\"Sample mean converges to {over_time[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f\"Sampling Distribution of the Sample Mean(Normalized; n={sample_size}; N={n_trials})\")\n",
    "sample_means.hist()\n",
    "printmd(\"$\\mu_{\\sigma}$ = \" + str(sample_means.std()))\n",
    "printmd(\"$\\mu_{\\overline{x}}$ = \" + str(sample_means.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Abs. Error vs. N Trials\")\n",
    "plt.xlabel(\"N Trials\")\n",
    "plt.ylabel(\"Abs Error\")\n",
    "plt.plot(error)\n",
    "printmd(f\"Abs. Error converges to {as_pct(error[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "mu = np.mean(sample_means)\n",
    "sigma = np.std(sample_means)\n",
    "plt.title(f\"Sample Distribution of Sample Mean (Normalized; n={k}; N={N})\")\n",
    "sample_means = pd.Series([(s - mu) / sigma for s in sample_means])\n",
    "sample_means.hist()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Read the whole text.\n",
    "text = \" \".join({w for w in words})\n",
    "\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(background_color=\"white\",max_font_size=40,scale=3).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "import matplotlib.pyplot as plt\n",
    "ax = plt.figure(figsize=(20,20))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# The pil way (if you don't have matplotlib)\n",
    "# image = wordcloud.to_image()\n",
    "# image.show()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
